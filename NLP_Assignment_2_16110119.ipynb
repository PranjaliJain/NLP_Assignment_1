{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "from nltk.probability import FreqDist,MLEProbDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import string\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "f=open(\"sherlock.txt\")\n",
    "f=f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[\\w\\?\\.\\!\\'-]+')\n",
    "tokens=tokenizer.tokenize(f)\n",
    "\n",
    "printable = set(string.printable)\n",
    "\n",
    "new_tokens = []\n",
    "for i,t in enumerate(tokens):\n",
    "    tokens[i] = filter(lambda x: x in printable , t)\n",
    "    tokens[i]=tokens[i].replace(\"'\",'')\n",
    "    tokens[i]=tokens[i].replace(\"--\",' ')\n",
    "    if tokens[i] != '':\n",
    "        new_tokens.append(tokens[i].lower())\n",
    "    \n",
    "corpus = ' '.join(new_tokens)\n",
    "\n",
    "startsentences = sent_tokenize(corpus)\n",
    "sentences=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding < S >  and  < /S >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in startsentences:\n",
    "    finalsentence = '<S> ' + sentence[:-1] + ' </S>'\n",
    "    sentences.append(finalsentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = sentences[0:(len(sentences)*8/10)]\n",
    "testing= sentences[(len(sentences)*8/10):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentences =' '.join(sentences)\n",
    "list_words = new_sentences.split(' ')\n",
    "list_words.remove('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Unigram MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneDict= FreqDist(list_words)\n",
    "unigrams = oneDict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oneDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneMLE = MLEProbDist(oneDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigramMLE={}\n",
    "for key in oneDict:\n",
    "    unigramMLE[key]=oneMLE.prob(key)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Bigram MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_list=[]\n",
    "flag=0\n",
    "for i in range(0,len(list_words)-1):\n",
    "    if list_words[i]==\"</S>\" and list_words[i+1]==\"<S>\":\n",
    "        flag=1\n",
    "    else:\n",
    "        bigrams_list.append((list_words[i],list_words[i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramDict = FreqDist(bigrams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramMLE={}\n",
    "for key in bigramDict:\n",
    "    d = oneDict[key[0]]\n",
    "    bigramMLE[key] = float(bigramDict[key])/d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Trigram MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_list=[]\n",
    "flag=0\n",
    "for i in range(0,len(list_words)-2):\n",
    "    if ((list_words[i]==\"</S>\" and list_words[i+1]==\"<S>\") or \n",
    "        (list_words[i+1]==\"</S>\" and list_words[i+2]==\"<S>\")):\n",
    "        flag=1\n",
    "    else:\n",
    "        trigrams_list.append((list_words[i],list_words[i+1], list_words[i+2]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigramDict = FreqDist(trigrams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigramMLE={}\n",
    "for key in trigramDict:\n",
    "    d = bigramDict[(key[0],key[1])]\n",
    "    trigramMLE[key] = float(trigramDict[key])/d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadgram MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "quadgrams_list=[]\n",
    "flag=0\n",
    "for i in range(0,len(list_words)-3):\n",
    "    if ((list_words[i]==\"</S>\" and list_words[i+1]==\"<S>\") or \n",
    "    (list_words[i+1]==\"</S>\" and list_words[i+2]==\"<S>\") or \n",
    "    (list_words[i+2]==\"</S>\" and list_words[i+3]==\"<S>\")):\n",
    "        flag=1\n",
    "    else:\n",
    "        quadgrams_list.append((list_words[i],list_words[i+1], list_words[i+2] , list_words[i+3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "quadgramDict = FreqDist(quadgrams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "quadgramMLE={}\n",
    "for key in quadgramDict:\n",
    "    d = trigramDict[(key[0],key[1],key[2])]\n",
    "    quadgramMLE[key] = float(quadgramDict[key])/d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of possible unigrams :  8298\n",
      "Number of actual unigrams in the corpus :  8298\n",
      "\n",
      "Number of possible bigrams :  34424253\n",
      "Number of actual bigrams in the corpus :  49315\n",
      "\n",
      "Number of possible trigrams :  95194534296\n",
      "Number of actual trigrams in the corpus :  84520\n",
      "\n",
      "Number of possible quadgrams :  197409665496330\n",
      "Number of actual quadgrams in the corpus :  93033\n"
     ]
    }
   ],
   "source": [
    "## Number of possible N-grams v/s actual number of N-grams in the corpus\n",
    "\n",
    "# Unigrams \n",
    "print \"\\nNumber of possible unigrams : \" , len(unigrams)\n",
    "print \"Number of actual unigrams in the corpus : \" , len(unigrams)\n",
    "\n",
    "# Bigrams\n",
    "print \"\\nNumber of possible bigrams : \" , (len(unigrams)*(len(unigrams) -1)/2)\n",
    "print \"Number of actual bigrams in the corpus : \" , len(bigramDict.keys())\n",
    "\n",
    "# Trigrams\n",
    "print \"\\nNumber of possible trigrams : \" , (len(unigrams)*(len(unigrams) -1)*(len(unigrams)-2)/6)\n",
    "print \"Number of actual trigrams in the corpus : \" , len(trigramDict.keys())\n",
    "\n",
    "# Quadgrams\n",
    "print \"\\nNumber of possible quadgrams : \" , (len(unigrams)*(len(unigrams) -1)*(len(unigrams)-2)*(len(unigrams)-3)/24)\n",
    "print \"Number of actual quadgrams in the corpus : \" , len(quadgramDict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator and Probability functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigramMLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator(model_name):\n",
    "    \n",
    "    gensentence = ['<S>']\n",
    "    gensenlength = 1\n",
    "    madesentence = ''\n",
    "    \n",
    "    \n",
    "    if model_name == 1:\n",
    "        \n",
    "        flag =1 \n",
    "\n",
    "        while(flag==1):    \n",
    "            nextword = np.random.choice(unigramMLE.keys(),1,p=unigramMLE.values())       \n",
    "            if nextword[0] == '</S>':\n",
    "                flag =0 \n",
    "            if nextword[0] == '<S>':\n",
    "                flag=1\n",
    "            else:\n",
    "                gensentence.append(nextword[0]) \n",
    "\n",
    "#########################################################################      \n",
    "    if model_name == 2:\n",
    "        flag=1\n",
    "        while(flag==1):\n",
    "            first={}\n",
    "            prob= {}\n",
    "            for bigram in (bigramMLE.keys()):            \n",
    "                if bigram not in first.keys() and bigram[0]==gensentence[len(gensentence)-1]:\n",
    "\n",
    "                    first[bigram] = bigramMLE[bigram]\n",
    "\n",
    "            for bigram in first:\n",
    "                prob[bigram[1]]= bigramMLE[bigram]\n",
    "\n",
    "            nextword = np.random.choice(prob.keys(),1,p=prob.values())\n",
    "            gensentence.append(nextword[0])\n",
    "            \n",
    "            if nextword[0] == '</S>':     \n",
    "                flag=0\n",
    "\n",
    "        \n",
    "############################################################################\n",
    "    if model_name == 3:\n",
    "        flag =1\n",
    "        \n",
    "        firstfrombigram={}\n",
    "        for bigram in (bigramMLE.keys()):            \n",
    "            if bigram not in firstfrombigram.keys() and bigram[0]=='<S>':\n",
    "                firstfrombigram[bigram[1]] = bigramMLE[bigram]\n",
    "             \n",
    "        nextword = np.random.choice(firstfrombigram.keys(),1,p=firstfrombigram.values())\n",
    "        gensentence.append(nextword[0])\n",
    "        \n",
    "        \n",
    "        while(flag==1):\n",
    "            first={}\n",
    "            prob= {}\n",
    "            for trigram in (trigramMLE.keys()):            \n",
    "                if trigram not in first.keys() and trigram[0]==gensentence[len(gensentence)-2] and trigram[1]==gensentence[len(gensentence)-1]:\n",
    "                    first[trigram] = trigramMLE[trigram]\n",
    "\n",
    "            for trigram in first:\n",
    "                prob[trigram[2]]= trigramMLE[trigram]\n",
    "\n",
    "            nextword = np.random.choice(prob.keys(),1,p=prob.values())\n",
    "            gensentence.append(nextword[0])\n",
    "            \n",
    "            if nextword[0] == '</S>':\n",
    "                flag=0\n",
    "    \n",
    "        \n",
    "############################################################################\n",
    "    if model_name == 4:\n",
    "        flag =1\n",
    "        \n",
    "        firstfrombigram={}\n",
    "        for bigram in (bigramMLE.keys()):            \n",
    "            if (bigram not in firstfrombigram.keys() and \n",
    "                bigram[0]=='<S>'):\n",
    "                firstfrombigram[bigram[1]] = bigramMLE[bigram]\n",
    "\n",
    "        nextword = np.random.choice(firstfrombigram.keys(),1,p=firstfrombigram.values())\n",
    "        gensentence.append(nextword[0])\n",
    "\n",
    "        secondfromtrigram = {}\n",
    "        for trigram in (trigramMLE.keys()):            \n",
    "            if (trigram not in secondfromtrigram.keys() and \n",
    "                trigram[0]=='<S>' and trigram[1]==gensentence[1]):\n",
    "                secondfromtrigram[trigram[2]] = trigramMLE[trigram]\n",
    "\n",
    "        nextword = np.random.choice(secondfromtrigram.keys(),1,p=secondfromtrigram.values())\n",
    "        gensentence.append(nextword[0])\n",
    "\n",
    "        while(flag==1):\n",
    "            first={}\n",
    "            prob= {}\n",
    "            for quadgram in (quadgramMLE.keys()):            \n",
    "                if (quadgram not in first.keys() and \n",
    "                quadgram[0]==gensentence[len(gensentence)-3] and \n",
    "                quadgram[1]==gensentence[len(gensentence)-2] and \n",
    "                quadgram[2]==gensentence[len(gensentence)-1]):\n",
    "                    \n",
    "                    first[quadgram] = quadgramMLE[quadgram]\n",
    "\n",
    "\n",
    "            for quadgram in first:\n",
    "                prob[quadgram[3]]= quadgramMLE[quadgram]\n",
    "\n",
    "            nextword = np.random.choice(prob.keys(),1,p=prob.values())\n",
    "            gensentence.append(nextword[0])\n",
    "            \n",
    "            if nextword[0] == '</S>':\n",
    "                flag=0\n",
    "\n",
    "\n",
    "############################################################################\n",
    "    \n",
    "    madesentence = ' '.join(gensentence)            \n",
    "    return madesentence\n",
    "                \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> well watson said holmes reaching up his hand though so he followed me there and he saw me without pa knowing anything about it </S>\n",
      "<S> you must know said he that i am still in the dark to see what would come of this strange affair </S>\n",
      "<S> he was a lawyer </S>\n",
      "<S> this he opened and made a pile while poor frank here had a claim that petered out and came to nothing </S>\n",
      "<S> in dress now for example </S>\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(Generator(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> i confess that the status of my trunk turned out the contents and drew from the king </S>\n",
      "<S> it was on board of a broad intelligent face sloping down to streatham since i saw the man who wrote it was something else which she must have almost gone and the ruffian who pursued me </S>\n",
      "<S> what do you mean </S>\n",
      "<S> you fail however to reason from what i wished to have taken it upstairs and a large scale and have this money or if they believed my statement for it was obvious from the little area of light now bright now faint as the old trick </S>\n",
      "<S> i cried </S>\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(Generator(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> inspector </S>\n",
      "<S> and i retired to the afternoon </S>\n",
      "<S> precisely i have seen a blind </S>\n",
      "<S> my dear watson said holmes are not heard a hundred yards from america with laudanum in his hand closed the matter up my neighbours who are getting these german i waited in the ring </S>\n",
      "<S> but have his life in the other consideration that so astute a demon ill state of the moonlight and benevolent curiosity </S>\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(Generator(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<S> there holmes </S>\n",
      "<S> to face little over forget i away the four need fortune who dear </S>\n",
      "<S> a </S>\n",
      "<S> me solemn the put i escape daring </S>\n",
      "<S> he notes the i my ive so as reached that witness was but him band were is that read beauty </S>\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(Generator(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigramMLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Probability(sentence,model_name):\n",
    "    sentence = sentence.lower()\n",
    "    new_sentence = '<S> ' + sentence + ' </S>'\n",
    "    listOFwords = new_sentence.split(' ')\n",
    "    ProbINlog= 0\n",
    "    \n",
    "    \n",
    "    if model_name ==1 : \n",
    "        \n",
    "        flag = 1\n",
    "        logprob=0\n",
    "        for Index in range(0,len(listOFwords)):\n",
    "            present = 0 \n",
    "            one = listOFwords[Index]\n",
    "            \n",
    "            if one in unigramMLE:\n",
    "                present = unigramMLE[one]\n",
    "                logprob = logprob + math.log(present)\n",
    "            else:\n",
    "                flag = 0\n",
    "                break\n",
    "        if flag == 0 :\n",
    "            print '\\nThis sentence is not present in corpus'\n",
    "        else : \n",
    "            ProbINlog = logprob\n",
    "            print '\\nThis sentence is present in corpus'\n",
    "        \n",
    "##########################################################################\n",
    "\n",
    "\n",
    "    if model_name == 2 :\n",
    "       \n",
    "        flag = 1\n",
    "        logprob=0\n",
    "        for Index in range(0,len(listOFwords)-1):\n",
    "            present = 0 \n",
    "            one = listOFwords[Index]\n",
    "            two = listOFwords[Index+1]\n",
    "            onetwo=(one,two)\n",
    "            \n",
    "            if onetwo in bigramMLE:\n",
    "                present = bigramMLE[onetwo]\n",
    "                logprob = logprob + math.log(present)\n",
    "            else:\n",
    "                flag = 0\n",
    "                break\n",
    "        if flag == 0 :\n",
    "            print '\\nThis sentence is not present in corpus'\n",
    "        else : \n",
    "            ProbINlog = logprob\n",
    "            print '\\nThis sentence is present in corpus'\n",
    "            \n",
    "            \n",
    "##########################################################################    \n",
    "    \n",
    "    \n",
    "    if model_name == 3 :\n",
    "        #print(listOFwords)\n",
    "        flag = 1\n",
    "        logprob=0\n",
    "        for Index in range(0,len(listOFwords)-2):\n",
    "            present = 0 \n",
    "            one = listOFwords[Index]\n",
    "            two = listOFwords[Index+1]\n",
    "            three = listOFwords[Index+2]\n",
    "            onetwothree=(one,two,three)\n",
    "            \n",
    "            if onetwothree in trigramMLE:\n",
    "                present = trigramMLE[onetwothree]\n",
    "                logprob = logprob + math.log(present)\n",
    "            else:\n",
    "                flag = 0\n",
    "                break\n",
    "        if flag == 0 :\n",
    "            print 'This sentence is not present in corpus'\n",
    "        else : \n",
    "            ProbINlog = logprob\n",
    "            print 'This sentence is present in corpus'\n",
    "            \n",
    "##########################################################################\n",
    "    \n",
    "    \n",
    "    if model_name == 4:\n",
    "        \n",
    "        flag = 1\n",
    "        logprob=0\n",
    "        for Index in range(0,len(listOFwords)-3):\n",
    "            present = 0 \n",
    "            one = listOFwords[Index]\n",
    "            two = listOFwords[Index+1]\n",
    "            three = listOFwords[Index+2]\n",
    "            four = listOFwords[Index+3]\n",
    "            onetwothreefour=(one,two,three,four)\n",
    "            \n",
    "            if onetwothreefour in quadgramMLE:\n",
    "                present = quadgramMLE[onetwothreefour]\n",
    "                logprob = logprob + math.log(present)\n",
    "            else:\n",
    "                flag = 0\n",
    "                break\n",
    "        if flag == 0 :\n",
    "            print '\\nThis sentence is not present in corpus'\n",
    "        else : \n",
    "            ProbINlog = logprob\n",
    "            print '\\nThis sentence is present in corpus'\n",
    "##########################################################################\n",
    "    \n",
    "    \n",
    "    if ProbINlog == 0 :\n",
    "        return 0\n",
    "    else:\n",
    "        return math.exp(ProbINlog)\n",
    "                \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This sentence is present in corpus\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.9044799151280424e-15"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Probability('not a bit doctor',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This sentence is not present in corpus\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Probability('this assignment is really long',4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add-1 smoothing for bigram model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_one(one,two):  # returns probability,effective count after add-1 smoothing \n",
    "    c = 0 \n",
    "    add_1bigramProb = 0\n",
    "    add_1effectivecount = 0\n",
    "    dv = oneDict[one] + len(unigrams)\n",
    "    d = oneDict[one] \n",
    "    \n",
    "    if (one,two) in bigramDict.keys():        \n",
    "        c=bigramDict[(one,two)]\n",
    "        add_1bigramProb = (float(c + 1))/dv\n",
    "    else:\n",
    "        add_1bigramProb = (float(1))/dv\n",
    "    \n",
    "    add_1effectivecount= (add_1bigramProb*d)\n",
    "    \n",
    "    return [add_1bigramProb , add_1effectivecount]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0002409058058299205, 0.000963623223319682]"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_one('eagerly', 'into')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.017686593562079942, 53.23664662186063]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_one('and','the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change in the count post add-1 smoothing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "someexamples=[('dressing-gown', 'his'),('was','a'),('laying', 'his'), ('eagerly', 'into'),('shed', 'in'),('and','the')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example  1\n",
      "Bigram :  ('dressing-gown', 'his')\n",
      "Count of bigram in corpus :  1\n",
      "Count of bigram after add-1 smoothing :  0.0014450867052\n",
      " \n",
      "\n",
      "Example  2\n",
      "Bigram :  ('was', 'a')\n",
      "Count of bigram in corpus :  160\n",
      "Count of bigram after add-1 smoothing :  23.3838071693\n",
      " \n",
      "\n",
      "Example  3\n",
      "Bigram :  ('laying', 'his')\n",
      "Count of bigram in corpus :  1\n",
      "Count of bigram after add-1 smoothing :  0.00168573148706\n",
      " \n",
      "\n",
      "Example  4\n",
      "Bigram :  ('eagerly', 'into')\n",
      "Count of bigram in corpus :  1\n",
      "Count of bigram after add-1 smoothing :  0.00096362322332\n",
      " \n",
      "\n",
      "Example  5\n",
      "Bigram :  ('shed', 'in')\n",
      "Count of bigram in corpus :  1\n",
      "Count of bigram after add-1 smoothing :  0.00024099289071\n",
      " \n",
      "\n",
      "Example  6\n",
      "Bigram :  ('and', 'the')\n",
      "Count of bigram in corpus :  199\n",
      "Count of bigram after add-1 smoothing :  53.2366466219\n",
      " \n"
     ]
    }
   ],
   "source": [
    "### Some examples are as follows : \n",
    "\n",
    "for i in someexamples:\n",
    "    \n",
    "    print '\\nExample ',(someexamples.index(i)+1)\n",
    "    print 'Bigram : ', i\n",
    "    print 'Count of bigram in corpus : ', bigramDict[i]\n",
    "    lst = add_one(i[0],i[1])\n",
    "    print 'Count of bigram after add-1 smoothing : ', lst[1]\n",
    "    print ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drastic change can be seen in the counts of bigrams after add-1 smoothing because some of the probability mass of bigrams present in the corpus gets distributed to bigrams that are not present in the corpus. Add-1 smoothing is used to consider bigrams that can be formed from the unigrams present in the corpus but these bigrams do not originally exist in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good-turing smoothing technique for bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencyOFcounts={}\n",
    "for i in bigramDict.values():\n",
    "    if i not in frequencyOFcounts:\n",
    "        frequencyOFcounts[i]=1\n",
    "    else:\n",
    "        frequencyOFcounts[i]= frequencyOFcounts[i] + 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 36863,\n",
       " 2: 5885,\n",
       " 3: 2234,\n",
       " 4: 1159,\n",
       " 5: 671,\n",
       " 6: 479,\n",
       " 7: 296,\n",
       " 8: 227,\n",
       " 9: 203,\n",
       " 10: 166,\n",
       " 11: 135,\n",
       " 12: 104,\n",
       " 13: 86,\n",
       " 14: 60,\n",
       " 15: 45,\n",
       " 16: 47,\n",
       " 17: 46,\n",
       " 18: 46,\n",
       " 19: 36,\n",
       " 20: 32,\n",
       " 21: 27,\n",
       " 22: 30,\n",
       " 23: 22,\n",
       " 24: 17,\n",
       " 25: 14,\n",
       " 26: 21,\n",
       " 27: 14,\n",
       " 28: 18,\n",
       " 29: 18,\n",
       " 30: 22,\n",
       " 31: 12,\n",
       " 32: 10,\n",
       " 33: 11,\n",
       " 34: 8,\n",
       " 35: 9,\n",
       " 36: 10,\n",
       " 37: 16,\n",
       " 38: 5,\n",
       " 39: 9,\n",
       " 40: 10,\n",
       " 41: 7,\n",
       " 42: 4,\n",
       " 43: 7,\n",
       " 44: 3,\n",
       " 45: 5,\n",
       " 46: 5,\n",
       " 47: 5,\n",
       " 48: 5,\n",
       " 49: 1,\n",
       " 50: 5,\n",
       " 51: 2,\n",
       " 52: 3,\n",
       " 53: 6,\n",
       " 54: 3,\n",
       " 55: 2,\n",
       " 56: 2,\n",
       " 57: 2,\n",
       " 59: 2,\n",
       " 60: 2,\n",
       " 61: 6,\n",
       " 63: 4,\n",
       " 65: 4,\n",
       " 66: 2,\n",
       " 68: 3,\n",
       " 70: 3,\n",
       " 71: 2,\n",
       " 74: 4,\n",
       " 75: 1,\n",
       " 76: 1,\n",
       " 77: 2,\n",
       " 79: 1,\n",
       " 80: 3,\n",
       " 81: 1,\n",
       " 82: 1,\n",
       " 83: 2,\n",
       " 84: 3,\n",
       " 86: 2,\n",
       " 88: 1,\n",
       " 90: 2,\n",
       " 92: 1,\n",
       " 93: 1,\n",
       " 95: 2,\n",
       " 96: 1,\n",
       " 97: 1,\n",
       " 98: 1,\n",
       " 99: 4,\n",
       " 102: 1,\n",
       " 103: 1,\n",
       " 104: 1,\n",
       " 106: 1,\n",
       " 108: 1,\n",
       " 109: 1,\n",
       " 110: 3,\n",
       " 114: 1,\n",
       " 115: 1,\n",
       " 116: 1,\n",
       " 117: 1,\n",
       " 118: 1,\n",
       " 120: 2,\n",
       " 122: 2,\n",
       " 123: 1,\n",
       " 125: 1,\n",
       " 130: 2,\n",
       " 131: 1,\n",
       " 134: 1,\n",
       " 135: 2,\n",
       " 136: 1,\n",
       " 138: 1,\n",
       " 139: 1,\n",
       " 140: 1,\n",
       " 142: 1,\n",
       " 143: 1,\n",
       " 149: 1,\n",
       " 151: 1,\n",
       " 154: 1,\n",
       " 158: 1,\n",
       " 160: 1,\n",
       " 168: 1,\n",
       " 169: 1,\n",
       " 182: 2,\n",
       " 184: 1,\n",
       " 186: 1,\n",
       " 191: 1,\n",
       " 196: 1,\n",
       " 199: 2,\n",
       " 201: 1,\n",
       " 211: 1,\n",
       " 237: 1,\n",
       " 249: 1,\n",
       " 260: 1,\n",
       " 279: 1,\n",
       " 301: 1,\n",
       " 304: 1,\n",
       " 335: 1,\n",
       " 336: 1,\n",
       " 346: 1,\n",
       " 419: 1,\n",
       " 505: 1,\n",
       " 713: 1,\n",
       " 997: 1}"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencyOFcounts        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "effectivecount1_10={}\n",
    "freqbetween1_10 = frequencyOFcounts.values()[:12]\n",
    "\n",
    "for i in range(0,10):\n",
    "    p = float((i+2)*freqbetween1_10[i+1])/freqbetween1_10[i]\n",
    "    effectivecount1_10[i+1]=p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstd=[]\n",
    "d_gt=0\n",
    "\n",
    "for key in effectivecount1_10:\n",
    "    lstd.append(key - effectivecount1_10[key])\n",
    "    \n",
    "d_gt = np.mean(lstd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of values of d for bigrams having original counts between 1 to 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6807096546672815,\n",
       " 0.8611724723874257,\n",
       " 0.9247985675917638,\n",
       " 1.1052631578947367,\n",
       " 0.7168405365126675,\n",
       " 1.674321503131524,\n",
       " 0.8648648648648649,\n",
       " -0.04845814977973539,\n",
       " 0.8226600985221673,\n",
       " 1.0542168674698793]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effective value of d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8656389573262576"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_turing(one,two):\n",
    "    c=0\n",
    "    gt_effectivecount=0\n",
    "    gt_bigramProb=0\n",
    "    d = oneDict[one] \n",
    "    \n",
    "    if (one,two) in bigramDict.keys():\n",
    "        c=bigramDict[(one,two)]\n",
    "        gt_bigramProb = float(c - d_gt)/d\n",
    "    else:\n",
    "        gt_bigramProb = float(frequencyOFcounts[1])/len(list_words)\n",
    "    return gt_bigramProb\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06582536911716735"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_turing('and','the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "train =' '.join(training)\n",
    "trainwords = train.split(' ')\n",
    "trainwords.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram counts\n",
    "unigramDictTrain= FreqDist(trainwords)\n",
    "unigramsTrain = unigramDictTrain.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram counts \n",
    "bigramlistTrain=[]\n",
    "flag=0\n",
    "for i in range(0,len(trainwords)-1):\n",
    "    if trainwords[i]==\"</S>\" and trainwords[i+1]==\"<S>\":\n",
    "        flag=1\n",
    "    else:\n",
    "        bigramlistTrain.append((trainwords[i],trainwords[i+1]))\n",
    "\n",
    "bigramDictTrain = FreqDist(bigramlistTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add-one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_oneTrain(one,two):  # returns probability,effective count after add-1 smoothing \n",
    "    c = 0 \n",
    "    add_1bigramProb = 0\n",
    "    \n",
    "    dv = unigramDictTrain[one] + len(unigramsTrain)\n",
    "    d = unigramDictTrain[one] \n",
    "    \n",
    "    if (one,two) in bigramDictTrain.keys():        \n",
    "        c=bigramDictTrain[(one,two)]\n",
    "        add_1bigramProb = (float(c + 1))/dv\n",
    "    else:\n",
    "        add_1bigramProb = (float(1))/dv\n",
    "    \n",
    "    return add_1bigramProb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00013157894736842105"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_oneTrain('sherlock','is')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good Turing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencyOFcountsTrain={}\n",
    "for i in bigramDictTrain.values():\n",
    "    if i not in frequencyOFcountsTrain:\n",
    "        frequencyOFcountsTrain[i]=1\n",
    "    else:\n",
    "        frequencyOFcountsTrain[i]= frequencyOFcountsTrain[i] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 31217,\n",
       " 2: 4917,\n",
       " 3: 1822,\n",
       " 4: 902,\n",
       " 5: 505,\n",
       " 6: 372,\n",
       " 7: 247,\n",
       " 8: 214,\n",
       " 9: 148,\n",
       " 10: 118,\n",
       " 11: 86,\n",
       " 12: 60,\n",
       " 13: 58,\n",
       " 14: 57,\n",
       " 15: 49,\n",
       " 16: 41,\n",
       " 17: 39,\n",
       " 18: 33,\n",
       " 19: 25,\n",
       " 20: 22,\n",
       " 21: 20,\n",
       " 22: 23,\n",
       " 23: 21,\n",
       " 24: 16,\n",
       " 25: 16,\n",
       " 26: 20,\n",
       " 27: 15,\n",
       " 28: 11,\n",
       " 29: 13,\n",
       " 30: 10,\n",
       " 31: 8,\n",
       " 32: 5,\n",
       " 33: 14,\n",
       " 34: 3,\n",
       " 35: 7,\n",
       " 36: 5,\n",
       " 37: 3,\n",
       " 38: 5,\n",
       " 39: 7,\n",
       " 40: 3,\n",
       " 41: 3,\n",
       " 42: 2,\n",
       " 43: 3,\n",
       " 44: 8,\n",
       " 45: 6,\n",
       " 46: 1,\n",
       " 47: 7,\n",
       " 48: 2,\n",
       " 49: 2,\n",
       " 50: 5,\n",
       " 52: 1,\n",
       " 53: 2,\n",
       " 54: 2,\n",
       " 55: 2,\n",
       " 56: 2,\n",
       " 57: 2,\n",
       " 58: 2,\n",
       " 59: 1,\n",
       " 60: 1,\n",
       " 62: 3,\n",
       " 63: 3,\n",
       " 64: 4,\n",
       " 66: 2,\n",
       " 68: 1,\n",
       " 69: 1,\n",
       " 70: 2,\n",
       " 71: 1,\n",
       " 72: 1,\n",
       " 73: 1,\n",
       " 74: 2,\n",
       " 76: 2,\n",
       " 77: 1,\n",
       " 78: 1,\n",
       " 79: 2,\n",
       " 81: 1,\n",
       " 85: 1,\n",
       " 86: 1,\n",
       " 88: 3,\n",
       " 89: 1,\n",
       " 90: 1,\n",
       " 91: 4,\n",
       " 92: 1,\n",
       " 93: 2,\n",
       " 96: 2,\n",
       " 97: 1,\n",
       " 98: 2,\n",
       " 101: 2,\n",
       " 102: 1,\n",
       " 105: 1,\n",
       " 107: 1,\n",
       " 108: 1,\n",
       " 112: 1,\n",
       " 114: 3,\n",
       " 118: 1,\n",
       " 120: 1,\n",
       " 121: 1,\n",
       " 124: 1,\n",
       " 125: 2,\n",
       " 127: 2,\n",
       " 130: 1,\n",
       " 137: 1,\n",
       " 139: 1,\n",
       " 145: 1,\n",
       " 147: 1,\n",
       " 148: 1,\n",
       " 154: 1,\n",
       " 167: 1,\n",
       " 170: 1,\n",
       " 172: 1,\n",
       " 184: 1,\n",
       " 188: 1,\n",
       " 197: 1,\n",
       " 218: 1,\n",
       " 235: 1,\n",
       " 257: 1,\n",
       " 270: 1,\n",
       " 274: 1,\n",
       " 285: 1,\n",
       " 340: 1,\n",
       " 393: 1,\n",
       " 586: 1,\n",
       " 748: 1}"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencyOFcountsTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "effectivecount1_10Train={}\n",
    "freqbetween1_10Train = frequencyOFcountsTrain.values()[:12]\n",
    "\n",
    "for i in range(0,10):\n",
    "    p = float((i+2)*freqbetween1_10Train[i+1])/freqbetween1_10Train[i]\n",
    "    effectivecount1_10Train[i+1]=p\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.3150206618188807,\n",
       " 2: 1.111653447223917,\n",
       " 3: 1.9802414928649836,\n",
       " 4: 2.7993348115299335,\n",
       " 5: 4.41980198019802,\n",
       " 6: 4.647849462365591,\n",
       " 7: 6.931174089068826,\n",
       " 8: 6.224299065420561,\n",
       " 9: 7.972972972972973,\n",
       " 10: 8.016949152542374}"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effectivecount1_10Train  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstdTrain=[]\n",
    "d_gtTrain=0\n",
    "\n",
    "for key in effectivecount1_10Train:\n",
    "    lstdTrain.append(key - effectivecount1_10Train[key])\n",
    "    \n",
    "dTrain=np.mean(lstdTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6849793381811193,\n",
       " 0.888346552776083,\n",
       " 1.0197585071350164,\n",
       " 1.2006651884700665,\n",
       " 0.5801980198019798,\n",
       " 1.352150537634409,\n",
       " 0.0688259109311744,\n",
       " 1.7757009345794392,\n",
       " 1.0270270270270272,\n",
       " 1.9830508474576263]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstdTrain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0580702863993943"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_turingTrain(one,two):\n",
    "    c=0\n",
    "    #gt_effectivecount=0\n",
    "    gt_bigramProb=0\n",
    "    d = unigramDictTrain[one] \n",
    "    \n",
    "    if (one,two) in bigramDictTrain.keys():\n",
    "        c=bigramDictTrain[(one,two)]\n",
    "        if bigramDictTrain[(one,two)]==1:\n",
    "            gt_bigramProb = float(c - lstdTrain[0])/d\n",
    "        else:            \n",
    "            gt_bigramProb = float(c - dTrain)/d\n",
    "    else:\n",
    "        gt_bigramProb = float(frequencyOFcountsTrain[1])/len(trainwords)\n",
    "\n",
    "    return gt_bigramProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33466267863077437"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_turingTrain('sherlock','is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0002340420964479054"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_turingTrain('that','anyone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =' '.join(testing)\n",
    "testwords = test.split(' ')\n",
    "testwords.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTbigramslist=[]\n",
    "\n",
    "flag=0\n",
    "for i in range(0,len(testwords)-1):\n",
    "    if testwords[i]==\"</S>\" and testwords[i+1]==\"<S>\":\n",
    "        flag=1\n",
    "    else:\n",
    "        TESTbigramslist.append((testwords[i],testwords[i+1]))\n",
    "\n",
    "TESTbigramDict = FreqDist(TESTbigramslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTbigramslist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity for add-one smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "probAddOne=0\n",
    "\n",
    "for i in range(0,len(testwords)-1):\n",
    "    temp = add_oneTrain(testwords[i],testwords[i+1])\n",
    "    probAddOne = probAddOne + math.log(temp)\n",
    "\n",
    "expprobAddOne = math.exp(probAddOne)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-186887.93498732225"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probAddOne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity for good turing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "probGoodTuring=0\n",
    "\n",
    "for i in range(0,len(testwords)-1):\n",
    "    temp = good_turingTrain(testwords[i],testwords[i+1])\n",
    "    probGoodTuring = probGoodTuring + math.log(temp)\n",
    "    \n",
    "expprobGoodTuring = math.exp(probGoodTuring)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-69781.18118438573"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probGoodTuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(denom):\n",
    "    temp=0\n",
    "    ppw=0\n",
    "    N=len(TESTbigramslist)\n",
    "    ppw = ((-1)*denom)/N\n",
    "    \n",
    "    return math.exp(ppw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Comparing perplexity of add-one and good turing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity of add-one for the test corpus is :  2726.93971843\n",
      "Perplexity of good turing for the test corpus is :  19.1791896829\n",
      "\n",
      "Lower the perplexity value for a test corpus, better the model\n",
      "Hence in this case good turing performs better\n"
     ]
    }
   ],
   "source": [
    "print '\\nPerplexity of add-one for the test corpus is : ',perplexity(probAddOne)\n",
    "print 'Perplexity of good turing for the test corpus is : ',perplexity(probGoodTuring)\n",
    "\n",
    "print '\\nLower the perplexity value for a test corpus, better the model'\n",
    "print 'Hence in this case good turing performs better'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
